{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u2/a2khatun/.conda/envs/grs/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output, display\n",
    " \n",
    "from analysis import get_model_list\n",
    "from post_process_save import get_cls_proper_model_list, get_fully_proper_model_list\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model_list = {\n",
    "        \"small\":[\"mistralai--Mistral-7B-v0.1\", \"Open-Orca--Mistral-7B-OpenOrca\", \"HuggingFaceH4--zephyr-7b-alpha\"],\n",
    "        \"large\":[\"meta-llama--Llama-2-13b-chat-hf\", \"meta-llama--Llama-2-70b-chat-hf\", \"garage-bAInd--Platypus2-70B-instruct\"],\n",
    "        \"openai\":[\"text-davinci-003\", \"gpt-3.5-turbo\", \"gpt-4\", \"gpt-4-1106-preview\"],\n",
    "    }\n",
    "\n",
    "pruned_models = sum(list(pruned_model_list.values()), [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "885"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../../../curated_datasets/version_3/data_v3.csv\", index_col=0)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NO                0.650847\n",
       "YES               0.168362\n",
       "Unknown           0.138983\n",
       "Yes in Fiction    0.041808\n",
       "Name: ground_truth, dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"ground_truth\"].value_counts(normalize=True) \n",
    "# unknown 13.89%, #known = 86.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What categories create confusion in the model? \n",
    "Here, confusion is defined as:\n",
    "- incorrect answers for statements that have known ground truth (per RT, per PT, per Category)\n",
    "- inconsistent responses for the same statement with different prompts. (+ sankey diagram for qualitative analyses)\n",
    "    - P0 vs P1-3 \n",
    "    - voting across prompts (2, 3, 4 votes)\n",
    "    - vote analysis across categories\n",
    "- Can model identify the difference between P3 and P4?\n",
    "    - quantify flipping (yes->no, C and D don't flip)\n",
    "    - effect of categories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorrect answers for statements that have known ground truth (per RT, per PT, per Category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revise_ground_truth(df, prompt_type):\n",
    "    df = df[df[\"ground_truth\"] != \"Unknown\"]\n",
    "\n",
    "    if prompt_type in [\"P1\", \"P2\"]: # sometimes P1,2 say Yes in Fiction. What about those? LLMs fault.\"\"\"\n",
    "        df[\"ground_truth\"] = df[\"ground_truth\"].replace({\"Yes in Fiction\": \"NO\"})\n",
    "    else:\n",
    "        df = df[df[\"ground_truth\"] != \"Yes in Fiction\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "def revise_response(df):\n",
    "    df[\"new_response\"] = df[\"new_response\"].replace({\"A\":\"YES\", \"B\":\"NO\"})\n",
    "    # we don't want to count GT conflict if model responses with anything other than Yes or No (Neither, C, D, Bad Output)\n",
    "    acceptable = [\"yes\", \"no\"]\n",
    "    df = df[df[\"new_response\"].str.lower().isin(acceptable)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def flip_p4_response(df):\n",
    "    df[\"new_response\"] = df[\"new_response\"].str.lower().replace({\"yes\":\"no\", \"no\":\"yes\"})\n",
    "    return df\n",
    "\n",
    "def get_response_column(cols):\n",
    "    if \"new_response\" in cols:\n",
    "        return \"new_response\"\n",
    "    elif \"response_trimmed\" in cols: # for non-mcq responses\n",
    "        return \"response_trimmed\"\n",
    "    elif \"text_response\" in cols: # for openai models\n",
    "        return \"text_response\"\n",
    "    else:\n",
    "        print(\"Error! Can't find response columns!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ground Truth conflict\n",
    "\n",
    "def get_conflict_df():\n",
    "    global_df = pd.DataFrame()\n",
    "    bo_or_count_df = pd.DataFrame()\n",
    "    prompt_types = [f\"P{i}\" for i in range(5)]\n",
    "\n",
    "    for model_size in [\"small\", \"large\", \"openai\"]:\n",
    "        if model_size == \"openai\":\n",
    "            directory = f\"model_responses/{model_size}\"\n",
    "            response_types = [\"2_options\", \"3_options\", \"4_options\"]\n",
    "            include_probs=False\n",
    "        else:\n",
    "            directory = f\"model_responses/{model_size}_model_runs/processed_model_responses_cls\"\n",
    "            response_types = [\"2_options\", \"3_options\", \"4_options\", \"4_options/option_probs\"]\n",
    "            include_probs=True\n",
    "        \n",
    "        model_list = get_model_list(directory)\n",
    "        proper_model_list = get_cls_proper_model_list(directory, model_list, include_probs)\n",
    "        print(model_size, len(proper_model_list))\n",
    "\n",
    "        for model in proper_model_list:\n",
    "            for response_type in response_types:\n",
    "                for prompt_type in prompt_types:\n",
    "                    filename = f\"{directory}/{model}/{response_type}/classification_response_{prompt_type}.csv\"\n",
    "                    df = pd.read_csv(filename, index_col=0)\n",
    "                    df[\"new_response\"] = df[get_response_column(df.columns)]\n",
    "                    \n",
    "                    df = revise_ground_truth(df, prompt_type)\n",
    "                    original_size = len(df)\n",
    "                    \n",
    "                    # Also get ratio of C, D, Neither, and BO to get a sense of model's accuracy\n",
    "                    # model has to have low of ^ these, and less ground truth conflict\n",
    "                    other_response = len(df[df[\"new_response\"].str.lower().isin([\"c\", \"d\", \"neither\"])])\n",
    "                    bad_output = len(df[df[\"new_response\"] == \"Bad Output\"])\n",
    "\n",
    "                    ## Add these for per category BO and OT counts\n",
    "                    df = df.assign(bo_count=lambda r: r[\"new_response\"] == \"Bad Output\") # Bad Output count\n",
    "                    df = df.assign(or_count=lambda r: r[\"new_response\"].str.lower().isin([\"c\", \"d\", \"neither\"])) # Other Response count\n",
    "                    df = df.assign(yes_no_count=lambda r: r[\"new_response\"].str.lower().isin([\"a\", \"b\", \"yes\", \"no\"])) # yes/no responses\n",
    "                    bo_or_df = df.groupby(\"tags\").agg({\"bo_count\":\"sum\", \"or_count\":\"sum\", \"yes_no_count\":\"sum\", \"new_response\":\"count\"}).T\n",
    "                    bo_or_df[\"prompt_type\"] = prompt_type\n",
    "                    bo_or_df[\"response_type\"] = response_type\n",
    "                    bo_or_df[\"model\"] = model\n",
    "                    bo_or_df[\"model_size\"] = model_size\n",
    "                    bo_or_count_df = pd.concat([bo_or_count_df, bo_or_df])\n",
    "\n",
    "                    df = revise_response(df) # removes BO and Other Responses\n",
    "                    if prompt_type == \"P4\":\n",
    "                        df = flip_p4_response(df)\n",
    "\n",
    "                    df = df.assign(conflict=lambda r: r[\"ground_truth\"].str.lower() != r[\"new_response\"].str.lower())\n",
    "                    category_conflicts = df.groupby(\"tags\").agg({\"conflict\":\"sum\"}).T\n",
    "                    \n",
    "                    category_conflicts[\"total_response\"] = original_size\n",
    "                    category_conflicts[\"total_conflict\"] = sum(df[\"conflict\"])\n",
    "                    category_conflicts[\"total_yes_no\"] = len(df)\n",
    "                    category_conflicts[\"other_response\"] = other_response\n",
    "                    category_conflicts[\"bad_output\"] = bad_output\n",
    "                    category_conflicts[\"prompt_type\"] = prompt_type\n",
    "                    category_conflicts[\"response_type\"] = response_type\n",
    "                    category_conflicts[\"model\"] = model\n",
    "                    category_conflicts[\"model_size\"] = model_size\n",
    "\n",
    "                    # display(category_conflicts)\n",
    "                    global_df = pd.concat([global_df, category_conflicts])\n",
    "                    assert other_response + bad_output + len(df) == original_size\n",
    "    \n",
    "    return global_df, bo_or_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small 26\n",
      "large 7\n",
      "openai 4\n"
     ]
    }
   ],
   "source": [
    "global_df, bo_or_count_df = get_conflict_df()\n",
    "global_df = global_df.reset_index(drop=True, names=[\"index\"]) # this is for when we only have conflict counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_df.to_csv(\"analysis_files/ground_truth_conflict.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(global_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_df = pd.read_csv(\"analysis_files/ground_truth_conflict.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_df = global_df[global_df[\"model\"].isin(pruned_models)]\n",
    "pruned_df[\"model\"] = pruned_df[\"model\"].apply(lambda x: x.split(\"--\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conflict_by_model = pruned_df.groupby([\"model_size\", \"model\"]).agg({\"total_conflict\":\"sum\", \"total_yes_no\":\"sum\" ,\"other_response\":\"sum\" ,\"bad_output\":\"sum\", \"total_response\":\"sum\"})\n",
    "\n",
    "conflict_by_model[\"conflict_percent\"] = conflict_by_model[\"total_conflict\"]*100/conflict_by_model[\"total_yes_no\"]\n",
    "conflict_by_model[\"accuracy\"] = 100-conflict_by_model[\"conflict_percent\"]\n",
    "\n",
    "conflict_by_model[\"yes_no_response_percent\"] = conflict_by_model[\"total_yes_no\"]*100/conflict_by_model[\"total_response\"]\n",
    "\n",
    "# for those that are supposed to be yes/no\n",
    "conflict_by_model[\"other_response_percent\"] = conflict_by_model[\"other_response\"]*100/conflict_by_model[\"total_response\"]\n",
    "\n",
    "# ALSO need to check across all statements, without unknown/yes in fiction filtering\n",
    "# for those that are supposed to be yes/no\n",
    "conflict_by_model[\"bad_output_percent\"] = conflict_by_model[\"bad_output\"]*100/conflict_by_model[\"total_response\"]\n",
    "\n",
    "df = conflict_by_model[[\"conflict_percent\", \"accuracy\", \"yes_no_response_percent\", \"other_response_percent\", \"bad_output_percent\"]].sort_values(\"conflict_percent\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[[\"accuracy\", \"other_response_percent\", \"bad_output_percent\"]].to_latex(index=True,float_format=\"{:.1f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "conflict_by_rt = pruned_df.groupby([\"model_size\", \"model\", \"response_type\"]).agg({\"total_conflict\":\"sum\", \"total_yes_no\":\"sum\" ,\"other_response\":\"sum\" ,\"bad_output\":\"sum\", \"total_response\":\"sum\"})\n",
    "conflict_by_rt[\"conflict_percent\"] = conflict_by_rt[\"total_conflict\"]*100/conflict_by_rt[\"total_yes_no\"]\n",
    "conflict_by_rt[\"accuracy\"] = 100-conflict_by_rt[\"conflict_percent\"]\n",
    "conflict_by_rt[\"yes_no_response_percent\"] = conflict_by_rt[\"total_yes_no\"]*100/conflict_by_rt[\"total_response\"]\n",
    "conflict_by_rt[\"other_response_percent\"] = conflict_by_rt[\"other_response\"]*100/conflict_by_rt[\"total_response\"]\n",
    "conflict_by_rt[\"bad_output_percent\"] = conflict_by_rt[\"bad_output\"]*100/conflict_by_rt[\"total_response\"]\n",
    "\n",
    "df = conflict_by_rt.reset_index().pivot_table(index=\"model\", columns=\"response_type\", values=\"accuracy\")\n",
    "df[\"diff\"] = df.max(axis=1) - df.min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.to_latex(index=True,float_format=\"{:.1f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "conflict_by_pt = pruned_df.groupby([\"model_size\", \"model\", \"prompt_type\"]).agg({\"total_conflict\":\"sum\", \"total_yes_no\":\"sum\" ,\"other_response\":\"sum\" ,\"bad_output\":\"sum\", \"total_response\":\"sum\"})\n",
    "conflict_by_pt[\"conflict_percent\"] = conflict_by_pt[\"total_conflict\"]*100/conflict_by_pt[\"total_yes_no\"]\n",
    "conflict_by_pt[\"accuracy\"] = 100-conflict_by_pt[\"conflict_percent\"]\n",
    "conflict_by_pt[\"yes_no_response_percent\"] = conflict_by_pt[\"total_yes_no\"]*100/conflict_by_pt[\"total_response\"]\n",
    "conflict_by_pt[\"other_response_percent\"] = conflict_by_pt[\"other_response\"]*100/conflict_by_pt[\"total_response\"]\n",
    "conflict_by_pt[\"bad_output_percent\"] = conflict_by_pt[\"bad_output\"]*100/conflict_by_pt[\"total_response\"]\n",
    "\n",
    "\n",
    "df = conflict_by_pt.reset_index().pivot_table(index=\"model\", columns=\"prompt_type\", values=\"accuracy\")\n",
    "df[\"diff\"] = df.max(axis=1) - df.min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.to_latex(index=True,float_format=\"{:.1f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find best combinations\n",
    "df = pruned_df[pruned_df[\"total_yes_no\"]/pruned_df[\"total_response\"] >= 0.7] # >=70\\% yes_no_repsonse\n",
    "df[\"accuracy\"] = 100 - df[\"total_conflict\"]*100/df[\"total_yes_no\"]\n",
    "df = df[[\"model\", \"response_type\", \"prompt_type\", \"accuracy\"]].sort_values(\"accuracy\", ascending=False)\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    _df = df.copy(True)\n",
    "    mp = {\"2_options\":\"2......................\", \n",
    "          \"3_options\":\"........3...............\", \n",
    "          \"4_options\":\"...............4........\", \n",
    "          \"4_options/option_probs\":\n",
    "                     \".......................4a\"}\n",
    "    _df[\"response_type\"] = _df[\"response_type\"].apply(lambda x: mp[x])\n",
    "    display(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head(10).to_latex(index=False,float_format=\"{:.1f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.tail(10).to_latex(index=False,float_format=\"{:.1f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.to_latex(index=False,float_format=\"{:.1f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_or_count_df = bo_or_count_df[bo_or_count_df[\"model\"].isin(pruned_models)]\n",
    "bo_or_count_df[\"model\"] = bo_or_count_df[\"model\"].apply(lambda x: x.split(\"--\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conflict by category\n",
    "conflict_by_ct = pruned_df.groupby([\"model\"]).agg({\"Conspiracy\":\"sum\", \"Controversy\":\"sum\" ,\"Fact\":\"sum\" ,\"Fiction\":\"sum\", \"Misconception\":\"sum\", \"Stereotype\":\"sum\"})\n",
    "yes_no_count_df = bo_or_count_df.loc[\"yes_no_count\"].groupby([\"model\"]).agg({\"Conspiracy\":\"sum\", \"Controversy\":\"sum\" ,\"Fact\":\"sum\" ,\"Fiction\":\"sum\", \"Misconception\":\"sum\", \"Stereotype\":\"sum\"})\n",
    "conflict_by_ct = conflict_by_ct*100/yes_no_count_df\n",
    "\n",
    "conflict_by_ct_df = conflict_by_ct[[\"Fact\", \"Conspiracy\", \"Controversy\", \"Misconception\", \"Stereotype\", \"Fiction\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(model\n",
       " Llama-2-13b-chat-hf        2.732201\n",
       " Llama-2-70b-chat-hf        4.185463\n",
       " Mistral-7B-OpenOrca       11.152448\n",
       " Mistral-7B-v0.1           26.180876\n",
       " Platypus2-70B-instruct     6.795469\n",
       " gpt-3.5-turbo              0.484620\n",
       " gpt-4                     10.230259\n",
       " gpt-4-1106-preview         7.544915\n",
       " text-davinci-003           8.418343\n",
       " zephyr-7b-alpha            7.513403\n",
       " dtype: float64,\n",
       " 8.523799723467741)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max - 2nd max\n",
    "d = conflict_by_ct_df.apply(max, axis=1) - conflict_by_ct_df.apply(lambda r: r[r.nlargest(2).index.values[1]], axis=1)\n",
    "d, d.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conflict_by_ct_df.to_latex(index=True,float_format=\"{:.1f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for each model and category pair, what is the least conflict in any given prompt or rt?\n",
    "## i.e what is the optimal number for each model-category pair?\n",
    "\n",
    "optimal_df = pruned_df.copy(True).set_index([\"model\", \"response_type\", \"prompt_type\"])[[\"Fact\", \"Conspiracy\", \"Controversy\", \"Misconception\", \"Stereotype\", \"Fiction\"]]\n",
    "yes_no_count_df = bo_or_count_df.loc[\"yes_no_count\"].set_index([\"model\", \"response_type\", \"prompt_type\"])[[\"Fact\", \"Conspiracy\", \"Controversy\", \"Misconception\", \"Stereotype\", \"Fiction\"]]\n",
    "optimal_df = optimal_df*100/yes_no_count_df\n",
    "optimal_df = optimal_df.reset_index()\n",
    "\n",
    "# shows which rt, pt had the minimum value in the df below\n",
    "idx_min_df = optimal_df.set_index([\"response_type\", \"prompt_type\"]).groupby(\"model\").idxmin()[[\n",
    "    \"Fact\", \n",
    "    \"Conspiracy\", \n",
    "    \"Controversy\", \n",
    "    \"Misconception\", \n",
    "    \"Stereotype\",\n",
    "    \"Fiction\", \n",
    "    ]]\n",
    "\n",
    "# display(idx_min_df)\n",
    "\n",
    "min_df = optimal_df.groupby(\"model\").min()[[\n",
    "    \"Fact\", \n",
    "    \"Conspiracy\", \n",
    "    \"Controversy\", \n",
    "    \"Misconception\", \n",
    "    \"Stereotype\",\n",
    "    \"Fiction\", \n",
    "    ]]\n",
    "\n",
    "min_df\n",
    "\n",
    "bo_and_other_response_df = idx_min_df.copy(deep=True)\n",
    "\n",
    "for col in idx_min_df.columns:\n",
    "    for ix, (rt, pt) in idx_min_df[col].items():\n",
    "        cat = col.split(\"_\")[0]\n",
    "        cond = (bo_or_count_df[\"model\"]==ix) & (bo_or_count_df[\"response_type\"]==rt) & (bo_or_count_df[\"prompt_type\"]==pt)\n",
    "        bo_count = bo_or_count_df[cond].loc[\"bo_count\", cat]\n",
    "        or_count = bo_or_count_df[cond].loc[\"or_count\", cat]\n",
    "        tot_count = bo_or_count_df[cond].loc[\"new_response\", cat]\n",
    "        bo_and_other_response_df.loc[ix, col] = f\"({bo_count*100/tot_count:.1f}, {or_count*100/tot_count:.1f})\"\n",
    "\n",
    "bo_and_other_response_df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = min_df.applymap(lambda x: f\"{x:.1f} \") + bo_and_other_response_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.to_latex(index=True).replace(\"(0.0\", \"(0\").replace(\" 0.0\", \" 0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [DONE] conflict by model, by pt across rt (which prompt is good per model)\n",
    "# conflict by pt across model and rt (which prompt is good?)\n",
    "\n",
    "# conflict by rt (across model, pt) (which rt is best?)\n",
    "# [DONE] conflict per rt per model (across pt)\n",
    "\n",
    "# conflict by category (across everythig else) (which category is most vulnerable)\n",
    "# [DONE] conflict by model by category (across pt and rt) (which category is vulnerable by model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inconsistent responses for the same statement with different prompts.\n",
    "Find number of statements that change responses with prompt (per category and overall).\n",
    "\n",
    "(+ sankey diagram and full-text for qualitative analyses)\n",
    "\n",
    "- P0 vs P1-3 \n",
    "- voting across prompts (2, 3, 4 votes)\n",
    "- vote analysis across categories\n",
    "\n",
    "Additionally:\n",
    "- P3 vs P4: did it flip?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P0 vs P1-3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small 26\n",
      "large 7\n",
      "openai 4\n",
      "small 26\n",
      "large 7\n",
      "openai 4\n"
     ]
    }
   ],
   "source": [
    "## Prompt consistency\n",
    "\n",
    "def load_df(filename):\n",
    "    df = pd.read_csv(filename, index_col=0)\n",
    "    df[\"new_response\"] = df[get_response_column(df.columns)]\n",
    "    return df\n",
    "\n",
    "def get_inter_prompt_conflict(ignore_yes_in_fiction, only_yes_in_fiction):\n",
    "    global_pt_conflict_df = pd.DataFrame()\n",
    "    \n",
    "    for model_size in [\"small\", \"large\", \"openai\"]:\n",
    "        if model_size == \"openai\":\n",
    "            directory = f\"model_responses/{model_size}\"\n",
    "            response_types = [\"2_options\", \"3_options\", \"4_options\"]\n",
    "            include_probs=False\n",
    "        else:\n",
    "            directory = f\"model_responses/{model_size}_model_runs/processed_model_responses_cls\"\n",
    "            response_types = [\"2_options\", \"3_options\", \"4_options\", \"4_options/option_probs\"]\n",
    "            include_probs=True\n",
    "        \n",
    "        model_list = get_model_list(directory)\n",
    "        proper_model_list = get_cls_proper_model_list(directory, model_list, include_probs)\n",
    "        print(model_size, len(proper_model_list))\n",
    "\n",
    "        for model in proper_model_list:\n",
    "            for response_type in response_types:\n",
    "\n",
    "                dir = f\"{directory}/{model}/{response_type}\"\n",
    "                \n",
    "                p0_df = load_df(f\"{dir}/classification_response_P0.csv\")\n",
    "                \n",
    "                if ignore_yes_in_fiction:\n",
    "                    p0_df = p0_df[p0_df[\"ground_truth\"]!=\"Yes in Fiction\"]\n",
    "                if only_yes_in_fiction:\n",
    "                    p0_df = p0_df[p0_df[\"ground_truth\"]==\"Yes in Fiction\"]\n",
    "                \n",
    "                # Get ratio BO to get a sense of model's accuracy\n",
    "                p0_df = p0_df.assign(p0_bad_output=lambda r: r[\"new_response\"] == \"Bad Output\")\n",
    "\n",
    "                for prompt_type in [\"P1\", \"P2\", \"P3\"]:\n",
    "                    df = load_df(f\"{dir}/classification_response_{prompt_type}.csv\")\n",
    "                    \n",
    "                    if ignore_yes_in_fiction:\n",
    "                        df = df[df[\"ground_truth\"]!=\"Yes in Fiction\"]\n",
    "                    if only_yes_in_fiction:\n",
    "                        df = df[df[\"ground_truth\"]==\"Yes in Fiction\"]\n",
    "                    \n",
    "                    # Get ratio BO to get a sense of model's accuracy\n",
    "                    bad_output = len(df[df[\"new_response\"] == \"Bad Output\"])\n",
    "                    df[\"p0_response\"] = p0_df[\"new_response\"]\n",
    "                    df[\"p0_bad_output\"] = p0_df[\"p0_bad_output\"]\n",
    "\n",
    "                    df = df.assign(p0_conflict=lambda r: r[\"p0_response\"].str.lower() != r[\"new_response\"].str.lower())\n",
    "                    df = df.assign(bad_output=lambda r: r[\"new_response\"] == \"Bad Output\")\n",
    "                    df = df.assign(both_bad_output=lambda r: (r[\"new_response\"] == \"Bad Output\") & (r[\"p0_response\"] == \"Bad Output\"))\n",
    "\n",
    "                    assert len(df) == len(p0_df)\n",
    "                    \n",
    "                    df[\"total\"] = \"\"\n",
    "                    conflicts = df.groupby(\"tags\").agg({\"p0_conflict\":\"sum\", \"p0_bad_output\":\"sum\", \"bad_output\":\"sum\", \"total\":\"count\", \"both_bad_output\":\"sum\"})\n",
    "                    conflicts[\"model_size\"] = model_size\n",
    "                    conflicts[\"model\"] = model\n",
    "                    conflicts[\"response_type\"] = response_type\n",
    "                    conflicts[\"prompt_type\"] = prompt_type\n",
    "\n",
    "                    # display(conflicts)\n",
    "                    global_pt_conflict_df = pd.concat([global_pt_conflict_df, conflicts])\n",
    "    \n",
    "    return global_pt_conflict_df\n",
    "\n",
    "global_pt_conflict_df = get_inter_prompt_conflict(True, False).reset_index() # ignore Yes in Fiction since it is SUPPOSED to conflict, for total fconflict calculation (by model and by rt)\n",
    "global_pt_conflict_df_yes_in_fiction = get_inter_prompt_conflict(False, True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_pt_conflict_df.to_csv(\"analysis_files/P0_conflict.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_pt_conflict_df = pd.read_csv(\"analysis_files/P0_conflict.csv\")\n",
    "global_pt_conflict_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_df = global_pt_conflict_df[global_pt_conflict_df[\"model\"].isin(pruned_models)]\n",
    "pruned_df[\"model\"] = pruned_df[\"model\"].apply(lambda x: x.split(\"--\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_conflict = pruned_df.pivot(\n",
    "    index = ['model_size', 'model', 'response_type', 'tags', 'total', 'p0_bad_output'],\n",
    "    columns = [\"prompt_type\"],\n",
    "    values = [\"p0_conflict\", 'bad_output', 'both_bad_output'],\n",
    ")\n",
    "\n",
    "pt_conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_conflict = pt_conflict.reset_index()\n",
    "pt_conflict.columns = ['model_size', 'model', 'response_type', 'tags', 'total', 'p0_bad_output', \n",
    "                       'p0_conflict_w_p1','p0_conflict_w_p2','p0_conflict_w_p3', \n",
    "                       'p1_bad_output', 'p2_bad_output', 'p3_bad_output',\n",
    "                       'p1_both_bad_output', 'p2_both_bad_output', 'p3_both_bad_output',\n",
    "                       ]\n",
    "pt_conflict.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brainstorm all questions\n",
    "\n",
    "set 1\n",
    "- [DONE] [x] which model is most inconsistent? (by model across rt, category, pt)\n",
    "- [x] which category is most consistent and inconsistent? (by category across model, rt, pt)\n",
    "- [] which prompt is closest to P0? (across model, category, rt)\n",
    "- [] ?? which rt produces most consistent / inconsistent responses?\n",
    "\n",
    "set 2\n",
    "- [DONE] [x] which prompt is most inconsistent by model? (acorss rt, category)\n",
    "- [DONE] [x] which category is most inconsistent by model? (across rt, pt)\n",
    "- [] ?? which rt is most inconsistent by model? (across category, pt)\n",
    "\n",
    "set 3\n",
    "- [x] which prompt is more/less conistent by model by rt? (across category)\n",
    "- [DONE][x] which prompt is more/less consistent by model by category? (across rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which prompt is more/less conistent by model by rt? (across category)\n",
    "pt_conflict_by_rt = pt_conflict.groupby(['model', 'response_type']).sum() #['p0_conflict_w_p1', 'p0_conflict_w_p2', 'p0_conflict_w_p3']\n",
    "\n",
    "# which prompt is most inconsistent by model? (acorss rt, category)\n",
    "pt_conflict_by_rt[\"all_pt_conflict\"] = pt_conflict_by_rt[\"p0_conflict_w_p1\"] \\\n",
    "                                        + pt_conflict_by_rt[\"p0_conflict_w_p2\"] \\\n",
    "                                        + pt_conflict_by_rt[\"p0_conflict_w_p3\"]\n",
    "\n",
    "pt_conflict_by_rt[\"conflict_percent\"] = pt_conflict_by_rt[\"all_pt_conflict\"]*100/(3*pt_conflict_by_rt[\"total\"]) # 3 for P1, P2, P3\n",
    "pt_conflict_by_rt[\"p1_conflict_percent\"] = pt_conflict_by_rt[\"p0_conflict_w_p1\"]*100/pt_conflict_by_rt[\"total\"]\n",
    "pt_conflict_by_rt[\"p2_conflict_percent\"] = pt_conflict_by_rt[\"p0_conflict_w_p2\"]*100/pt_conflict_by_rt[\"total\"]\n",
    "pt_conflict_by_rt[\"p3_conflict_percent\"] = pt_conflict_by_rt[\"p0_conflict_w_p3\"]*100/pt_conflict_by_rt[\"total\"]\n",
    "\n",
    "# pt_conflict_by_rt = pt_conflict_by_rt.sort_values(\"conflict_percent\")\n",
    "display(pt_conflict_by_rt.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pt_conflict_by_rt[['conflict_percent']].reset_index()#.sort_values(\"conflict_percent\")\n",
    "df = df.pivot_table(index=\"model\", columns=\"response_type\", values=\"conflict_percent\")\n",
    "df\n",
    "print(df.to_latex(index=True,float_format=\"{:.1f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which model is most inconsistent? (by model across rt, category, pt)\n",
    "pt_conflict_by_model = pt_conflict.groupby(['model_size', 'model']).sum()\n",
    "\n",
    "# which prompt is most inconsistent by model? (acorss rt, category)\n",
    "pt_conflict_by_model[\"all_pt_conflict\"] = pt_conflict_by_model[\"p0_conflict_w_p1\"] \\\n",
    "                                        + pt_conflict_by_model[\"p0_conflict_w_p2\"] \\\n",
    "                                        + pt_conflict_by_model[\"p0_conflict_w_p3\"]\n",
    "\n",
    "pt_conflict_by_model[\"all_bo\"] = pt_conflict_by_model[\"p1_both_bad_output\"] \\\n",
    "                                        + pt_conflict_by_model[\"p2_both_bad_output\"] \\\n",
    "                                        + pt_conflict_by_model[\"p3_both_bad_output\"]\n",
    "\n",
    "pt_conflict_by_model = pt_conflict_by_model.sort_values(\"all_pt_conflict\")\n",
    "display(pt_conflict_by_model.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_conflict_by_model[\"bo_percent\"] = pt_conflict_by_model[\"all_bo\"]*100/(3*pt_conflict_by_model[\"total\"]) # 3 for P1, P2, P3\n",
    "pt_conflict_by_model[\"conflict_percent\"] = pt_conflict_by_model[\"all_pt_conflict\"]*100/(3*pt_conflict_by_model[\"total\"]) # 3 for P1, P2, P3\n",
    "pt_conflict_by_model[\"p1_conflict_percent\"] = pt_conflict_by_model[\"p0_conflict_w_p1\"]*100/pt_conflict_by_model[\"total\"]\n",
    "pt_conflict_by_model[\"p2_conflict_percent\"] = pt_conflict_by_model[\"p0_conflict_w_p2\"]*100/pt_conflict_by_model[\"total\"]\n",
    "pt_conflict_by_model[\"p3_conflict_percent\"] = pt_conflict_by_model[\"p0_conflict_w_p3\"]*100/pt_conflict_by_model[\"total\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pt_conflict_by_model[['p1_conflict_percent', 'p2_conflict_percent', 'p3_conflict_percent', 'conflict_percent', 'bo_percent']].sort_values(\"conflict_percent\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.to_latex(index=True,float_format=\"{:.1f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which category is most consistent and inconsistent? (by category across model, rt, pt)\n",
    "# pt_conflict_by_category = pt_conflict.groupby(['tags']).sum()\n",
    "\n",
    "# pt_conflict_by_category[\"all_pt_conflict\"] = pt_conflict_by_category[\"p0_conflict_w_p1\"] \\\n",
    "#                                         + pt_conflict_by_category[\"p0_conflict_w_p2\"] \\\n",
    "#                                         + pt_conflict_by_category[\"p0_conflict_w_p3\"]\n",
    "\n",
    "# pt_conflict_by_category = pt_conflict_by_category.sort_values(\"all_pt_conflict\")\n",
    "# display(pt_conflict_by_category.head(10))\n",
    "\n",
    "\n",
    "# which category is most inconsistent by model? (across rt, pt)\n",
    "pt_conflict_by_category_by_model = pt_conflict.groupby(['model', 'tags']).sum()\n",
    "\n",
    "pt_conflict_by_category_by_model[\"all_pt_conflict\"] = pt_conflict_by_category_by_model[\"p0_conflict_w_p1\"] \\\n",
    "                                        + pt_conflict_by_category_by_model[\"p0_conflict_w_p2\"] \\\n",
    "                                        + pt_conflict_by_category_by_model[\"p0_conflict_w_p3\"]\n",
    "\n",
    "pt_conflict_by_category_by_model[\"conflict_percent\"] = pt_conflict_by_category_by_model[\"all_pt_conflict\"]*100/(pt_conflict_by_category_by_model[\"total\"]*3)\n",
    "pt_conflict_by_category_by_model[\"p1_conflict_percent\"] = pt_conflict_by_category_by_model[\"p0_conflict_w_p1\"]*100/pt_conflict_by_category_by_model[\"total\"]\n",
    "pt_conflict_by_category_by_model[\"p2_conflict_percent\"] = pt_conflict_by_category_by_model[\"p0_conflict_w_p2\"]*100/pt_conflict_by_category_by_model[\"total\"]\n",
    "pt_conflict_by_category_by_model[\"p3_conflict_percent\"] = pt_conflict_by_category_by_model[\"p0_conflict_w_p3\"]*100/pt_conflict_by_category_by_model[\"total\"]\n",
    "\n",
    "display(pt_conflict_by_category_by_model.head(12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pt_conflict_by_category_by_model[['p1_conflict_percent', 'p2_conflict_percent','p3_conflict_percent', 'conflict_percent']]\\\n",
    "    .reset_index()\n",
    "\n",
    "DF = pd.DataFrame()\n",
    "\n",
    "for model, group_df in df.groupby(\"model\"):\n",
    "    del group_df[\"model\"]\n",
    "    group_df.set_index(\"tags\")\n",
    "    group_df = group_df.pivot_table(columns=[\"tags\"])\n",
    "    group_df[\"model\"] = model\n",
    "    DF = pd.concat([DF, group_df])\n",
    "\n",
    "df = DF.reset_index().rename(columns={\"index\":\"conflict\"}).set_index([\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df[df[\"conflict\"]!=\"conflict_percent\"][[\"conflict\", \"Fact\", \"Conspiracy\", \"Controversy\", \"Misconception\", \"Stereotype\", \"Fiction\"]]\n",
    "print(d.to_latex(index=True,float_format=\"{:.1f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [\"Fact\", \"Conspiracy\", \"Controversy\", \"Misconception\", \"Stereotype\", \"Fiction\"]\n",
    "for cat in cats:\n",
    "    # display(df.reset_index().set_index(\"conflict\").groupby(\"model\")[cat].idxmin()) # which prompt has the least conflict in each category\n",
    "    display(df.reset_index().set_index(\"conflict\").groupby(\"model\")[cat].idxmax()) # which prompt has the most conflict in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df[df[\"conflict\"]==\"conflict_percent\"][[\"Fact\", \"Conspiracy\", \"Controversy\", \"Misconception\", \"Stereotype\", \"Fiction\"]]\n",
    "print(d.to_latex(index=True,float_format=\"{:.1f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_conflict_by_category_by_model = pt_conflict_by_category_by_model.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## analyze this: is there a common most conflict category for most models?\n",
    "common_category = pt_conflict_by_category_by_model.loc[\n",
    "    pt_conflict_by_category_by_model.groupby(['model'])[\"conflict_percent\"].idxmax()][['model', \"tags\", \"conflict_percent\"]\n",
    "]\n",
    "display(common_category)\n",
    "\n",
    "display(common_category[\"tags\"].value_counts())\n",
    "\n",
    "\"\"\"\n",
    "Fiction          15\n",
    "Misconception     8\n",
    "Fact              7\n",
    "Stereotype        7\n",
    "\"\"\"\n",
    "# This means Fiction is the category is the most conflict in 15 models\n",
    "# Conspiracy and Controversy are never the 'most conflict' in any models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only prompt conflict in Yes in Fiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = global_pt_conflict_df_yes_in_fiction[global_pt_conflict_df_yes_in_fiction[\"model\"].isin(pruned_models)]\n",
    "df[\"model\"] = df[\"model\"].apply(lambda x: x.split(\"--\")[-1])\n",
    "\n",
    "df = df.pivot(\n",
    "    index = ['model_size', 'model', 'response_type', 'tags', 'total', 'p0_bad_output'],\n",
    "    columns = [\"prompt_type\"],\n",
    "    values = [\"p0_conflict\", 'bad_output', 'both_bad_output'],\n",
    ")\n",
    "\n",
    "df = df.reset_index()\n",
    "df.columns = ['model_size', 'model', 'response_type', 'tags', 'total', 'p0_bad_output', \n",
    "            'p0_conflict_w_p1','p0_conflict_w_p2','p0_conflict_w_p3', \n",
    "            'p1_bad_output', 'p2_bad_output', 'p3_bad_output',\n",
    "            'p1_both_bad_output', 'p2_both_bad_output', 'p3_both_bad_output',\n",
    "            ]\n",
    "display(df.head(2))\n",
    "\n",
    "\n",
    "# which model is most inconsistent? (by model across rt, category, pt)\n",
    "pt_conflict_by_model = df.groupby(['model']).sum()\n",
    "\n",
    "# which prompt is most inconsistent by model? (acorss rt, category)\n",
    "pt_conflict_by_model[\"all_pt_conflict\"] = pt_conflict_by_model[\"p0_conflict_w_p1\"] \\\n",
    "                                        + pt_conflict_by_model[\"p0_conflict_w_p2\"] \\\n",
    "                                        + pt_conflict_by_model[\"p0_conflict_w_p3\"]\n",
    "\n",
    "pt_conflict_by_model[\"all_bo\"] = pt_conflict_by_model[\"p1_both_bad_output\"] \\\n",
    "                                        + pt_conflict_by_model[\"p2_both_bad_output\"] \\\n",
    "                                        + pt_conflict_by_model[\"p3_both_bad_output\"]\n",
    "\n",
    "pt_conflict_by_model = pt_conflict_by_model.sort_values(\"all_pt_conflict\")\n",
    "display(pt_conflict_by_model.head(10))\n",
    "\n",
    "pt_conflict_by_model[\"bo_percent\"] = pt_conflict_by_model[\"all_bo\"]*100/(3*pt_conflict_by_model[\"total\"]) # 3 for P1, P2, P3\n",
    "pt_conflict_by_model[\"conflict_percent\"] = pt_conflict_by_model[\"all_pt_conflict\"]*100/(3*pt_conflict_by_model[\"total\"]) # 3 for P1, P2, P3\n",
    "pt_conflict_by_model[\"p1_conflict_percent\"] = pt_conflict_by_model[\"p0_conflict_w_p1\"]*100/pt_conflict_by_model[\"total\"]\n",
    "pt_conflict_by_model[\"p2_conflict_percent\"] = pt_conflict_by_model[\"p0_conflict_w_p2\"]*100/pt_conflict_by_model[\"total\"]\n",
    "pt_conflict_by_model[\"p3_conflict_percent\"] = pt_conflict_by_model[\"p0_conflict_w_p3\"]*100/pt_conflict_by_model[\"total\"]\n",
    "\n",
    "df = pt_conflict_by_model[['p1_conflict_percent', 'p2_conflict_percent', 'p3_conflict_percent', 'conflict_percent', 'bo_percent']].sort_values(\"conflict_percent\")\n",
    "display(df)\n",
    "\n",
    "df = pt_conflict_by_model[['p1_conflict_percent', 'p2_conflict_percent', 'p3_conflict_percent', 'conflict_percent']]\n",
    "print(df.to_latex(index=True,float_format=\"{:.1f}\".format))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting across prompts and categories\n",
    "- voting across prompts (2, 3, 4 votes)\n",
    "- vote analysis across categories\n",
    "\n",
    "Qs:\n",
    "- [TODO] [ ] per model\n",
    "- [x] vote per model, per rt, per category\n",
    "- [TODO] [x] per model per cateory\n",
    "- [x] per category (across models and rt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small 26\n",
      "large 7\n",
      "openai 4\n"
     ]
    }
   ],
   "source": [
    "## Prompt consistency\n",
    "\n",
    "global_vote_df = pd.DataFrame()\n",
    "\n",
    "def load_df(filename):\n",
    "    df = pd.read_csv(filename, index_col=0)\n",
    "    df[\"new_response\"] = df[get_response_column(df.columns)]\n",
    "    return df\n",
    "\n",
    "for model_size in [\"small\", \"large\", \"openai\"]:\n",
    "    if model_size == \"openai\":\n",
    "        directory = f\"model_responses/{model_size}\"\n",
    "        response_types = [\"2_options\", \"3_options\", \"4_options\"]\n",
    "        include_probs=False\n",
    "    else:\n",
    "        directory = f\"model_responses/{model_size}_model_runs/processed_model_responses_cls\"\n",
    "        response_types = [\"2_options\", \"3_options\", \"4_options\", \"4_options/option_probs\"]\n",
    "        include_probs=True\n",
    "    \n",
    "    model_list = get_model_list(directory)\n",
    "    proper_model_list = get_cls_proper_model_list(directory, model_list, include_probs)\n",
    "    print(model_size, len(proper_model_list))\n",
    "\n",
    "    for model in proper_model_list:\n",
    "        for response_type in response_types:\n",
    "            cols = ['text', 'tags', 'sub_tags', 'ground_truth', 'paper link', 'data_source']\n",
    "            intermediate_df = load_df(f\"{directory}/{model}/{response_type}/classification_response_P0.csv\")[cols]\n",
    "\n",
    "            for prompt_type in [\"P0\", \"P1\", \"P2\", \"P3\", \"P4\"]:\n",
    "                df = load_df(f\"{directory}/{model}/{response_type}/classification_response_{prompt_type}.csv\")\n",
    "                assert len(intermediate_df[cols].compare(df[cols])) == 0 # no difference\n",
    "                intermediate_df[prompt_type + \"_response\"] = df[\"new_response\"]\n",
    "            \n",
    "            intermediate_df[\"model_size\"] = model_size\n",
    "            intermediate_df[\"model\"] = model\n",
    "            intermediate_df[\"response_type\"] = response_type\n",
    "            \n",
    "            global_vote_df = pd.concat([global_vote_df, intermediate_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vote(row):\n",
    "    mp = {}\n",
    "    for i in range(4):\n",
    "        col = f\"P{i}_response\"\n",
    "        val = row[col]\n",
    "        if val in mp:\n",
    "            mp[val] += 1\n",
    "        else:\n",
    "            mp[val] = 1\n",
    "\n",
    "    return max(list(mp.values()))\n",
    "\n",
    "global_vote_df[\"vote\"] = global_vote_df.apply(count_vote, axis=1)\n",
    "# vote = 1 means every prompt have a different output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_vote_df.to_csv(\"analysis_files/vote_across_prompts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "122112"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_vote_df = pd.read_csv(\"analysis_files/vote_across_prompts.csv\")\n",
    "print(len(global_vote_df))\n",
    "\n",
    "global_vote_df = global_vote_df[global_vote_df[\"ground_truth\"]!=\"Yes in Fiction\"]\n",
    "len(global_vote_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_df = global_vote_df[global_vote_df[\"model\"].isin(pruned_models)]\n",
    "pruned_df[\"model\"] = pruned_df[\"model\"].apply(lambda x: x.split(\"--\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per category (across models and rt) \n",
    "per_cat = pruned_df.groupby([\"vote\", \"tags\"]).agg({\"text\":\"count\"}).rename(columns={\"text\":\"count\"})\n",
    "# display(per_cat)\n",
    "\n",
    "## sort by category, rename legend title\n",
    "g = sns.catplot(\n",
    "    data=per_cat.reset_index(), kind=\"bar\",\n",
    "    x=\"vote\", y=\"count\", hue=\"tags\",\n",
    "    hue_order=[\"Fact\", \"Conspiracy\", \"Controversy\", \"Misconception\", \"Fiction\", \"Stereotype\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pruned_df.groupby(\"model\").agg({\"vote\":\"count\"})\n",
    "df = pruned_df.groupby([\"model\", \"vote\"]).size().unstack(fill_value=0)\n",
    "df[\"total\"] = df.sum(axis=1)\n",
    "for i in range(1,5):\n",
    "    df[i] = df[i]*100/df[\"total\"]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[[1,2,3,4]].sort_values(4, ascending=False).to_latex(index=True,float_format=\"{:.1f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per model per cateory\n",
    "per_cat_per_model = pruned_df.groupby([\"model_size\", \"model\", \"vote\", \"tags\"]).agg({\"text\":\"count\"}).rename(columns={\"text\":\"Count\"})\n",
    "per_cat_per_model = per_cat_per_model.groupby([\"model_size\", \"model\", \"tags\"]).apply(lambda x:100 * x / float(x.sum()))\n",
    "# display(per_cat)\n",
    "\n",
    "## sort by category, rename legend title\n",
    "def draw_per_model_per_cat_vote(model_size, col_order):\n",
    "    df = per_cat_per_model.reset_index().rename(columns={\"tags\":\"Categories\", \"vote\":\"Vote Count\", \"model\":\"Model\"})\n",
    "    df = df[df[\"model_size\"]==model_size]\n",
    "    g = sns.catplot(\n",
    "        data=df, kind=\"bar\",\n",
    "        x=\"Vote Count\", y=\"Count\", hue=\"Categories\", col=\"Model\", #row=\"model\",\n",
    "        hue_order=[\"Fact\", \"Conspiracy\", \"Controversy\", \"Misconception\", \"Stereotype\", \"Fiction\"],\n",
    "        col_order=col_order\n",
    "    )\n",
    "    g.set(ylim=(0, 100))\n",
    "    g.set_axis_labels(y_var=\"% vote frequency by category\")\n",
    "    # return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_per_model_per_cat_vote(\"openai\", col_order=['text-davinci-003','gpt-3.5-turbo','gpt-4','gpt-4-1106-preview'])\n",
    "draw_per_model_per_cat_vote(\"large\", col_order=['Llama-2-13b-chat-hf','Llama-2-70b-chat-hf','Platypus2-70B-instruct'])\n",
    "draw_per_model_per_cat_vote(\"small\", col_order=['Mistral-7B-v0.1','Mistral-7B-OpenOrca','zephyr-7b-alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vote per model, per rt, per category\n",
    "per_cat_per_model_per_rt = pruned_df.groupby([\"model_size\", \"model\", \"tags\", \"response_type\", \"vote\"]).agg({\"text\":\"count\"}).rename(columns={\"text\":\"Count\"})\n",
    "per_cat_per_model_per_rt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_per_rt_per_cat_vote(model):\n",
    "    df = per_cat_per_model_per_rt.reset_index().rename(columns={\"tags\":\"Categories\", \"vote\":\"Vote Count\", \"model\":\"Model\", \"response_type\":\"Response Type\"})\n",
    "    df = df[df[\"Model\"]==model]\n",
    "    g = sns.catplot(\n",
    "        data=df, kind=\"bar\",\n",
    "        x=\"Vote Count\", y=\"Count\", hue=\"Categories\", col=\"Response Type\", #row=\"model\",\n",
    "        hue_order=[\"Fact\", \"Conspiracy\", \"Controversy\", \"Misconception\", \"Stereotype\", \"Fiction\"],\n",
    "    )\n",
    "    g.figure.subplots_adjust(top=0.8)\n",
    "    g.figure.suptitle(model)\n",
    "    g.savefig(f\"analysis_files/figs/per_rt_vote/per_model_per_rt_per_cat_vote_{model}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in pruned_models:\n",
    "    model = model.split(\"--\")[-1]\n",
    "    draw_per_rt_per_cat_vote(model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per model per category count of 4 votes\n",
    "\n",
    "df = pruned_df.groupby([\"model\",\"tags\", \"vote\"]).size().unstack(fill_value=0)\n",
    "df[\"total\"] = df.sum(axis=1)\n",
    "for i in range(1,5):\n",
    "    df[i] = df[i]*100/df[\"total\"]\n",
    "\n",
    "df = df[[4]].reset_index().pivot_table(index=\"model\", columns=\"tags\", values=4)[[\"Fact\", \"Conspiracy\", \"Controversy\", \"Misconception\", \"Stereotype\", \"Fiction\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.to_latex(index=True,float_format=\"{:.1f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per model per rt count of 4 votes\n",
    "\n",
    "df = pruned_df.groupby([\"model\",\"response_type\", \"vote\"]).size().unstack(fill_value=0)\n",
    "df[\"total\"] = df.sum(axis=1)\n",
    "for i in range(1,5):\n",
    "    df[i] = df[i]*100/df[\"total\"]\n",
    "\n",
    "df = df[[4]].reset_index().pivot_table(index=\"model\", columns=\"response_type\", values=4)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.to_latex(index=True,float_format=\"{:.1f}\".format))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P3 vs P4 (Did it flip?)\n",
    "From p3->p4\n",
    "- Neither, C, D, should remain the same\n",
    "- BO in p3 or p4 should be ignored\n",
    "- Yes/A should become No/B and vice versa\n",
    "\n",
    "\"flipped\" -> higher is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_vote_df = pd.read_csv(\"analysis_files/vote_across_prompts.csv\")\n",
    "global_vote_df = global_vote_df[global_vote_df[\"ground_truth\"]!=\"Yes in Fiction\"]\n",
    "\n",
    "global_vote_df = global_vote_df[global_vote_df[\"model\"].isin(pruned_models)]\n",
    "global_vote_df[\"model\"] = global_vote_df[\"model\"].apply(lambda x: x.split(\"--\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HANDLE BO. SO far:\n",
    "- Accuracy: BO is ignored. Only yes/no responses are considered. *This means is a model produces all BO, and 1/2 correct answer, it gets to be 100% accurate, which is not good.*\n",
    "- Consistency: BO is NOT ignored. This mant BO--BO was considered consistent, Yes/NO-BO is inconsistent. *This means a model gets points for producing BO-BO, which is not good? maybe?*\n",
    "- Resolution: BO is ignored. Only yes/no is considered. *This means if a model produces all BO and 1/2 yes-no, it is considerd 100% correct, which is not good.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore BO\n",
    "flip_df = global_vote_df[[\"model_size\", \"model\", \"response_type\", \"tags\", \"ground_truth\", \"P3_response\", \"P4_response\"]].copy(deep=True)\n",
    "flip_df = flip_df[(flip_df[\"P3_response\"]!=\"Bad Output\") & (flip_df[\"P4_response\"]!=\"Bad Output\")]\n",
    "## ALSO SHOW THE NUMBER OF BO. Because we dont want to give a model points if it keeps produces \n",
    "# BO while we penalize a model for producing a reasonable but incorrect answer\n",
    "\n",
    "def is_cd_good(p3, p4):\n",
    "    # both should be c or d or Neither\n",
    "    return p3 in [\"Neither\", \"C\", \"D\"] and p3==p4\n",
    "\n",
    "def is_ab_good(p3, p4):\n",
    "    # Yes to No or vice versa\n",
    "    # A to B or vie versa\n",
    "    mp = {\n",
    "        \"A\":\"Yes\",\n",
    "        \"YES\":\"Yes\",\n",
    "        \"Yes\":\"Yes\",\n",
    "        \"B\":\"No\",\n",
    "        \"NO\":\"No\",\n",
    "        \"No\":\"No\",\n",
    "    }\n",
    "    opposite_mp = {\n",
    "        \"Yes\":\"No\",\n",
    "        \"No\":\"Yes\"\n",
    "    }\n",
    "    return p3 in mp and p4 in mp and mp[p3]==opposite_mp[mp[p4]]\n",
    "\n",
    "flip_df[\"cd_good\"] = flip_df.apply(lambda r: is_cd_good(r[\"P3_response\"], r[\"P4_response\"]), axis=1)\n",
    "flip_df[\"ab_good\"] = flip_df.apply(lambda r: is_ab_good(r[\"P3_response\"], r[\"P4_response\"]), axis=1)\n",
    "\n",
    "flip_df[\"flipped\"] = (flip_df[\"cd_good\"]+flip_df[\"ab_good\"]).astype(int)\n",
    "flip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per model, per rt, per tag flip\n",
    "df = flip_df.groupby([\"model_size\", \"model\", \"response_type\", \"tags\"]).agg({\"flipped\":[\"sum\", \"count\"]})\n",
    "df = df.reset_index()\n",
    "df.columns = [\"model_size\", \"model\", \"response_type\", \"tags\", \"flipped_count\", \"total_count\"] # here total is total among which flipped is correct\n",
    "df.to_csv(\"analysis_files/p3_flipped.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per model\n",
    "per_model_flip = flip_df.groupby([\"model\"]).agg({\"flipped\":[\"sum\", \"count\"]})\n",
    "per_model_flip.columns = [\"flipped_sum\", \"should_flip_count\"]\n",
    "per_model_flip[\"percent\"] = per_model_flip[\"flipped_sum\"]*100/per_model_flip[\"should_flip_count\"]\n",
    "\n",
    "per_model_flip.reset_index().to_csv(\"analysis_files/p3_flipped_by_model.csv\", index=False)\n",
    "\n",
    "per_model_flip.sort_values(\"percent\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find percent of BO\n",
    "bo_df = global_vote_df[[\"model_size\", \"model\", \"response_type\", \"tags\", \"ground_truth\", \"P3_response\", \"P4_response\"]].copy(deep=True)\n",
    "bo_df[\"bo\"] = (bo_df[\"P3_response\"]==\"Bad Output\") | (bo_df[\"P4_response\"]==\"Bad Output\")\n",
    "bo_df = bo_df.groupby([\"model\"]).agg({\"bo\":[\"sum\", \"count\"]})\n",
    "bo_df.columns = ['_'.join(col).strip() for col in bo_df.columns.values]\n",
    "bo_df[\"bo_percent\"] = bo_df[\"bo_sum\"]*100/bo_df[\"bo_count\"]\n",
    "bo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_model_flip[\"bo_output\"] = bo_df[\"bo_percent\"]\n",
    "per_model_flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = per_model_flip[[\"percent\", \"bo_output\"]].sort_values(\"percent\", ascending=False)\n",
    "print(df.to_latex(index=True,float_format=\"{:.1f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per model, per tag flip\n",
    "flip_df[\"tags\"] = pd.Categorical(flip_df[\"tags\"], [\"Fact\", \"Conspiracy\", \"Controversy\", \"Misconception\", \"Fiction\", \"Stereotype\"])\n",
    "per_model_per_tag_flip = flip_df.groupby([\"model\", \"tags\"]).agg({\"flipped\":[\"sum\", \"count\"]})\n",
    "per_model_per_tag_flip.columns = [\"flipped_sum\", \"should_flip_count\"]\n",
    "per_model_per_tag_flip[\"percent\"] = per_model_per_tag_flip[\"flipped_sum\"]*100/per_model_per_tag_flip[\"should_flip_count\"]\n",
    "# per_model_per_tag_flip = per_model_per_tag_flip.sort_values(\"percent\", ascending=False)\n",
    "\n",
    "# Find percent of BO\n",
    "bo_df = global_vote_df[[\"model_size\", \"model\", \"response_type\", \"tags\", \"ground_truth\", \"P3_response\", \"P4_response\"]].copy(deep=True)\n",
    "bo_df[\"bo\"] = (bo_df[\"P3_response\"]==\"Bad Output\") | (bo_df[\"P4_response\"]==\"Bad Output\")\n",
    "bo_df = bo_df.groupby([\"model\", \"tags\"]).agg({\"bo\":[\"sum\", \"count\"]})\n",
    "bo_df.columns = ['_'.join(col).strip() for col in bo_df.columns.values]\n",
    "bo_df[\"bo_percent\"] = bo_df[\"bo_sum\"]*100/bo_df[\"bo_count\"]\n",
    "\n",
    "per_model_per_tag_flip[\"bo_percent\"] = bo_df[\"bo_percent\"]\n",
    "per_model_per_tag_flip[\"bo_count\"] = bo_df[\"bo_sum\"]\n",
    "per_model_per_tag_flip[\"total_response_count\"] = bo_df[\"bo_count\"]\n",
    "# bo_sum + should_flip_count == total_response_count\n",
    "\n",
    "per_model_per_tag_flip.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = per_model_per_tag_flip[[\"percent\", \"bo_percent\"]]\n",
    "df[\"string\"] = df.apply(lambda r: f\"{r['percent']:.1f} ({r['bo_percent']:.1f})\", axis=1)\n",
    "df[\"string\"] = df[\"string\"].astype(str)\n",
    "df = df[[\"string\"]]\n",
    "df = df.reset_index().set_index(\"tags\")\n",
    "df = df.reset_index().pivot_table(index=\"model\", columns=\"tags\", aggfunc=lambda x: x)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.to_latex(index=True,float_format=\"{:.1f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per model, per rt flip\n",
    "per_model_per_rt_flip = flip_df.groupby([\"model\", \"response_type\"]).agg({\"flipped\":[\"sum\", \"count\"]})\n",
    "per_model_per_rt_flip.columns = [\"flipped_sum\", \"should_flip_count\"]\n",
    "per_model_per_rt_flip[\"percent\"] = per_model_per_rt_flip[\"flipped_sum\"]*100/per_model_per_rt_flip[\"should_flip_count\"]\n",
    "\n",
    "# Find percent of BO\n",
    "bo_df = global_vote_df[[\"model_size\", \"model\", \"response_type\", \"tags\", \"ground_truth\", \"P3_response\", \"P4_response\"]].copy(deep=True)\n",
    "bo_df[\"bo\"] = (bo_df[\"P3_response\"]==\"Bad Output\") | (bo_df[\"P4_response\"]==\"Bad Output\")\n",
    "bo_df = bo_df.groupby([\"model\", \"response_type\"]).agg({\"bo\":[\"sum\", \"count\"]})\n",
    "bo_df.columns = ['_'.join(col).strip() for col in bo_df.columns.values]\n",
    "bo_df[\"bo_percent\"] = bo_df[\"bo_sum\"]*100/bo_df[\"bo_count\"]\n",
    "\n",
    "per_model_per_rt_flip[\"bo_percent\"] = bo_df[\"bo_percent\"]\n",
    "per_model_per_rt_flip[\"bo_count\"] = bo_df[\"bo_sum\"]\n",
    "per_model_per_rt_flip[\"total_response_count\"] = bo_df[\"bo_count\"]\n",
    "# bo_sum + should_flip_count == total_response_count\n",
    "\n",
    "df = per_model_per_rt_flip[[\"percent\", \"bo_percent\"]]\n",
    "df[\"string\"] = df.apply(lambda r: f\"{r['percent']:.1f} ({r['bo_percent']:.1f})\", axis=1)\n",
    "df[\"string\"] = df[\"string\"].astype(str)\n",
    "df = df[[\"string\"]]\n",
    "df = df.reset_index().set_index(\"response_type\")\n",
    "df = df.reset_index().pivot_table(index=\"model\", columns=\"response_type\", aggfunc=lambda x: x)\n",
    "display(df.head())\n",
    "print(df.to_latex(index=True,float_format=\"{:.1f}\".format))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model confidence (MCQ prob)\n",
    "Only consider models that did not randomize and/or have realtively good combined probability.\n",
    "- How do confidence scores change for responses that remained the same. \"Number of statements with ≥20% points absolute change of confidence as compared to prompt 0.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
